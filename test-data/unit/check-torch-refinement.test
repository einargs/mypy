[case testTorchRefinementMnist]
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from typing_extensions import Annotated
from refinement import RefinementVar

T = RefinementVar('T')
S = RefinementVar('S')

A = RefinementVar('A')
B = RefinementVar('B')

R = RefinementVar('R')
V = RefinementVar('V')

X0 = RefinementVar('X0')
X1 = RefinementVar('X1')
X2 = RefinementVar('X2')
X3 = RefinementVar('X3')
X4 = RefinementVar('X4')
X5 = RefinementVar('X5')
X6 = RefinementVar('X6')
X7 = RefinementVar('X7')
X8 = RefinementVar('X8')
X9 = RefinementVar('X9')
X10 = RefinementVar('X10')

class Net(nn.Module):
    def __init__(self) -> None:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, (3,3), (1,1), (0,0), (1,1))
        self.conv2 = nn.Conv2d(32, 64, (3,3), (1,1), (0,0), (1,1))
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(
            self,
            x: Annotated[Tensor, T, T.shape == (64, 1, 28, 28)]
    ) -> Annotated[Tensor, R, R.shape == (64, 10)]:
        x0: Annotated[Tensor, X0, X0.shape == (64, 32, 26, 26)] = self.conv1(x)
        x1: Annotated[Tensor, X1, X1.shape == (64, 32, 26, 26)] = F.relu(x0)
        x2: Annotated[Tensor, X2, X2.shape == (64, 64, 24, 24)] = self.conv2(x1)
        x3: Annotated[Tensor, X3, X3.shape == (64, 64, 24, 24)] = F.relu(x2)
        x4: Annotated[Tensor, X4, X4.shape == (64, 64, 12, 12)] = F.max_pool2d(x3, (2,2), (2,2), (0,0), (1,1))
        x5: Annotated[Tensor, X5, X5.shape == (64, 64, 12, 12)] = self.dropout1(x4)
        x6: Annotated[Tensor, X6, X6.shape == (64, 9216)] = torch.flatten(x5, 1, 4)
        x7: Annotated[Tensor, X7, X7.shape == (64, 128)] = self.fc1(x6)
        x8: Annotated[Tensor, X8, X8.shape == (64, 128)] = F.relu(x7)
        x9: Annotated[Tensor, X9, X9.shape == (64, 128)] = self.dropout2(x8)
        x10: Annotated[Tensor, X10, X10.shape == (64, 10)] = self.fc2(x9)
        return F.log_softmax(x10, dim=1)

    def forward2(
            self,
            x: Annotated[Tensor, T, T.shape == (64, 1, 28, 28)]
    ) -> Annotated[Tensor, R, R.shape == (64, 10)]:
        x0 = self.conv1(x)
        x1 = F.relu(x0)
        x2 = self.conv2(x1)
        x3 = F.relu(x2)
        x4 = F.max_pool2d(x3, (2,2), (2,2), (0,0), (1,1))
        x5 = self.dropout1(x4)
        x6 = torch.flatten(x5, start=1, end=4)
        x7 = self.fc1(x6)
        x8 = F.relu(x7)
        x9 = self.dropout2(x8)
        x10 = self.fc2(x9)
        return F.log_softmax(x10, dim=1)

[builtins fixtures/primitives.pyi]

[case testTorchRefinementMinimalMnist]
from torch import Tensor
import torch.nn as nn
from typing_extensions import Annotated
from refinement import RefinementVar

T = RefinementVar('T')

X0 = RefinementVar('X0')

class Net(nn.Module):
    def __init__(self) -> None:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, (3,3), (1,1), (0,0), (1,1))
        self.conv2 = nn.Conv2d(32, 64, (3,3), (1,1), (0,0), (1,1))
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(
            self,
            x: Annotated[Tensor, T, T.shape == (64, 1, 28, 28)]
    ) -> None:
        x0: Annotated[Tensor, X0, X0.shape == (64, 32, 26, 26)] = self.conv1(x)
[builtins fixtures/primitives.pyi]

[case testTorchRefinementReUseMnist]
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from typing_extensions import Annotated
from refinement import RefinementVar

T = RefinementVar('T')
S = RefinementVar('S')

A = RefinementVar('A')
B = RefinementVar('B')

R = RefinementVar('R')
V = RefinementVar('V')

class Net(nn.Module):
    def __init__(self) -> None:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, (3,3), (1,1), (0,0), (1,1))
        self.conv2 = nn.Conv2d(32, 64, (3,3), (1,1), (0,0), (1,1))
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(
            self,
            input: Annotated[Tensor, T, T.shape == (64, 1, 28, 28)]
    ) -> Annotated[Tensor, R, R.shape == (64, 10)]:
        x = self.conv1(input)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, (2,2), (2,2), (0,0), (1,1))
        x = self.dropout1(x)
        x = torch.flatten(x, 1, 4)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

[builtins fixtures/primitives.pyi]

[case testTorchRefinementFlatten]
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from typing_extensions import Annotated
from refinement import RefinementVar

T = RefinementVar('T')
S = RefinementVar('S')

A = RefinementVar('A')
B = RefinementVar('B')

R = RefinementVar('R')
V = RefinementVar('V')

X0 = RefinementVar('X0')
X1 = RefinementVar('X1')
X2 = RefinementVar('X2')
X3 = RefinementVar('X3')
X4 = RefinementVar('X4')
X5 = RefinementVar('X5')
X6 = RefinementVar('X6')
X7 = RefinementVar('X7')
X8 = RefinementVar('X8')
X9 = RefinementVar('X9')
X10 = RefinementVar('X10')

def block0(t: Annotated[Tensor, T, T.shape == (64, 64, 12, 12)]) -> None:
    s: Annotated[Tensor, S, S.shape == (64, 9216)] = torch.flatten(t, start=1, end=4)

class Net(nn.Module):
    def __init__(self) -> Annotated[None, V]:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, (3,3), (1,1), (0,0), (1,1))
        self.conv2 = nn.Conv2d(32, 64, (3,3), (1,1), (0,0), (1,1))
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(
            self,
            x: Annotated[Tensor, T, T.shape == (64, 1, 28, 28)]
    ) -> None:
        x0: Annotated[Tensor, X0, X0.shape == (64, 32, 26, 26)] = self.conv1(x)
        x1: Annotated[Tensor, X1, X1.shape == (64, 32, 26, 26)] = F.relu(x0)
        x2: Annotated[Tensor, X2, X2.shape == (64, 64, 24, 24)] = self.conv2(x1)
        x3: Annotated[Tensor, X3, X3.shape == (64, 64, 24, 24)] = F.relu(x2)
        x4: Annotated[Tensor, X4, X4.shape == (64, 64, 12, 12)] = F.max_pool2d(x3, (2,2), (2,2), (0,0), (1,1))
        x5: Annotated[Tensor, X5, X5.shape == (64, 64, 12, 12)] = self.dropout1(x4)
        x6: Annotated[Tensor, X6, X6.shape == (64, 9216)] = torch.flatten(x5, 1, 4)
[builtins fixtures/primitives.pyi]

[case testTorchRefinementUseExpand]
import torch
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
from typing_extensions import Annotated
from refinement import RefinementVar

T = RefinementVar('T')
R = RefinementVar('R')

class Net(nn.Module):
    def __init__(self) -> None:
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1, 0, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1, 0, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(
            self,
            input: Annotated[Tensor, T, T.shape == (64, 1, 28, 28)]
    ) -> Annotated[Tensor, R, R.shape == (64, 10)]:
        x = self.conv1(input)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2, 2, 0, 1)
        x = self.dropout1(x)
        x = torch.flatten(x, 1, 4)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)
[builtins fixtures/primitives.pyi]
